# -*- coding: utf-8 -*-
"""multilayerPerceptron.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mKjmpuEcQFvh1D_NZC5v3w3eBV55Z_dO

#### Import Statements
"""

import torch
from torch import nn
from torch.utils.data import DataLoader
from torchvision import datasets
from torchvision.transforms import ToTensor
import numpy

import tqdm
import matplotlib.pyplot as plt

# Download data
training_data = datasets.FashionMNIST(
    root='data', 
    train=True, 
    download=True, 
    transform=ToTensor()
)

test_data = datasets.FashionMNIST(
    root='data', 
    train=False, 
    download=True, 
    transform=ToTensor()
)

batch_size=128

# Create data loader
train_dataloader = DataLoader(training_data, batch_size=batch_size)
test_dataloader = DataLoader(test_data, batch_size=batch_size)

# load one batch of training data
for data in train_dataloader:
  break

x = data[0]
y = data[1]
print('Shape of X [N, C, H, W]: ', x.shape)
print('Shape of Y: ', y.shape)

plt.figure(figsize=(8,10))
for i in range(25): 
  plt.subplot(5,5,i+1)
  plt.imshow(x[i,0,:,:], cmap="gray")
  plt.title("Label:%i" %y[i])

"""#### Define MLP Modelrs
The model will have three layers
- Input layer shape: 28X28X1
- Hidden layer shape: 512
- Output layer shape: 10
"""

class MLP(nn.Module):
  # struture of the model
  def __init__(self):
    super(MLP, self).__init__()
    self.flatten = nn.Flatten()
    self.linear_relu_stack = nn.Sequential(
        # first layer
        nn.Linear(28*28, 512),
        # non-linear activation
        nn.ReLU(), 
        # 2nd layer
        nn.Linear(512,512), 
        nn.ReLU(),
        # 3rd layer
        nn.Linear(512, 10)
    )
    
  # data passing flow
  def forward(self, x):
    x = self.flatten(x)
    logits = self.linear_relu_stack(x)
    return logits

"""#### Define the Training Operation
- Normally, we train a NN in multiple epochs
  - Epoch: the model sees the entire training data once
- For training, we need: 
  - NN model
  - Loss Function (cost)
  - Optimizer (e.g., gradient descent)
- Tasks for one training step (i.e., batch)
  - Load one batch
  - Pass the batch through the model
  - Get the model output
  - Compute the cost (i.e., loss or prediction error)
  - Backpropagation 
  
"""

def train(dataloader, model, loss_fn, optimizer, device):
  model.train() # set model to train model
  for step, (x, y) in enumerate(dataloader): 
    # send data to GPU or CPU
    x = x.to(device)
    y = y.to(device)
    
    # feed the data to the model
    pred = model(x)
    
    # compute the loss
    loss = loss_fn(pred, y)
    
    # backpropagation (update the parameters)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    
    if step % 100 == 0: 
      loss  = loss.item()
      print('Current Step: %d, loss:%.4f' %(step, loss))

def test(dataloader, model, loss_fn, device):
  num_batch = len(dataloader)

  model.eval()
  test_loss = 0
  correct = 0

  with torch.no_grad():
    for x, y in dataloader:
      x = x.to(device)
      y = y.to(device)
      pred = model(x)
      loss = loss_fn(pred, y)
      test_loss += loss.item()
      
      y_hat = pred.argmax(1)
      correct_batch = (y_hat == y).type(torch.float).sum().item()
      correct += correct_batch
  test_loss /= num_batch
  correct = correct / (num_batch * batch_size)

  print("Test Accuracy:%.4f" % correct)

# get CPU or GPU for training
device = "cuda" if torch.cuda.is_available() else "cpu"
print("Using %s device" %device)

# create the model
model = MLP().to(device)
print(model)

# optimizing the model parameter
loss_fn = nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)

# train the model in epochs
epochs = 5
for t in tqdm.tqdm(range(epochs)):
  print("Epoch %d \n----------------------" %t)
  train(train_dataloader, model, loss_fn, optimizer, device)
  test(test_dataloader, model, loss_fn, device)
print("Done!")

"""#### Training Results
- Model 1 (512, 512, 10)
  - Training time: 1 minute 49 seconds
  - Accuracy: 56%
"""

# visualize the testing result
classes = [
    "T-Shirt/Top", 
    "Trouser",
    "Pullover", 
    "Dress", 
    "Coat", 
    "Sandal", 
    "Shirt", 
    "Sneaker", 
    "Bag", 
    "Ankle Boot",
]

model.eval()
for x, y in test_dataloader:
  x = x.to(device)
  y = y.to(device)
  with torch.no_grad(): 
    pred = model(x)
    pred_labels = pred.argmax(1)
  break

plt.figure(figsize=(15,18))
for i in range(25): 
  y_hat = pred_labels[i].item()
  y_gt = y[i].item()
  plt.subplot(5,5,i+1)
  plt.imshow(x[i,0,:,:].cpu().numpy(), cmap="gray")
  plt.title('Predicted Label:%s\nGL Label:%s' %(classes[y_hat], classes[y_gt]))